{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim \n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size    = 220000\n",
    "embedding_dim = 50\n",
    "batch_size    = 256\n",
    "input_len     = 36\n",
    "epochs        = 10\n",
    "print_every   = 1000\n",
    "cuda          = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_files():\n",
    "    with open('../data/sentiment_data.pkl', 'rb') as data_file:\n",
    "        data = pickle.load(data_file)\n",
    "\n",
    "    with open('../data/sentiment_vocabulary.pkl', 'rb') as vocab_file:\n",
    "        vocab = pickle.load(vocab_file)\n",
    "        \n",
    "    return data, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_to_idx(vocab):\n",
    "    items       = list(vocab.items())\n",
    "    items       = sorted(items, key = lambda x: x[1], reverse = True)\n",
    "    word_to_idx = {word : i + 1 for i, (word, _) in enumerate(items[:vocab_size])}\n",
    "    \n",
    "    return word_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(data, word_to_idx, input_len):\n",
    "    encoded_data = []\n",
    "    \n",
    "    for tweet, target in data:\n",
    "        encoded_tweet = [word_to_idx.get(word, 0) for word in tweet]\n",
    "        len_encoding  = len(encoded_tweet) \n",
    "        if len(encoded_tweet) < input_len:\n",
    "            encoded_tweet = encoded_tweet + [0] * (input_len - len_encoding)\n",
    "        else:\n",
    "            encoded_tweet = encoded_tweet[:input_len]\n",
    "        encoded_data.append((' '.join(tweet), encoded_tweet, target))\n",
    "        \n",
    "    return encoded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(vocab_size, input_len, test_proportion = 0.2):\n",
    "    data, vocab   = load_files()\n",
    "    word_to_idx   = create_word_to_idx(vocab)\n",
    "    encoded_data  = encode_data(data, word_to_idx, input_len)\n",
    "    training_size = int(len(encoded_data) * (1 - test_proportion))  \n",
    "    random.shuffle(encoded_data)\n",
    "    training_data = encoded_data[:training_size]\n",
    "    test_data     = encoded_data[training_size:]\n",
    "    \n",
    "    return training_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_to_tensor(batch):\n",
    "    tweets  = [tweet for tweet, _, _ in batch]\n",
    "    inputs  = torch.LongTensor([input for _, input, _ in batch])\n",
    "    targets = torch.LongTensor([target for _, _, target in batch])\n",
    "    \n",
    "    return tweets, inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(data, batch_size, shuffle = True):\n",
    "    if shuffle:\n",
    "        data = random.sample(data, len(data))\n",
    "        \n",
    "    return (batch_to_tensor(data[i: i + batch_size]) for i in range(0, len(data), batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(cnn, criterion, train_data, test_data, batch_size):\n",
    "    def evaluate_model_data(data):\n",
    "        batch_number     = 0\n",
    "        total_loss       = 0\n",
    "        total_correct    = 0\n",
    "        total_prediction = 0\n",
    "        for _, inputs, targets in batch_generator(data, batch_size, shuffle = False):\n",
    "            inputs            = Variable(inputs)\n",
    "            targets           = Variable(targets)\n",
    "            inputs            = inputs.cuda() if cuda else inputs\n",
    "            targets           = targets.cuda() if cuda else targets\n",
    "            predictions       = cnn(inputs)\n",
    "            loss              = criterion(predictions, targets)\n",
    "            total_loss       += loss.cpu().data[0]\n",
    "            batch_number     += 1\n",
    "            pred_classes      = predictions.max(dim = 1)[1]\n",
    "            total_prediction += predictions.size()[0]\n",
    "            total_correct    += (pred_classes == targets).cpu().sum().data[0]\n",
    "        average_loss     = total_loss / batch_number\n",
    "        average_accuracy = total_correct / total_prediction\n",
    "        \n",
    "        return average_loss, average_accuracy\n",
    "    \n",
    "    return evaluate_model_data(train_data), evaluate_model_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_evaluation(cnn, epoch, criterion, train_data, test_data, batch_size):\n",
    "    cnn.eval()\n",
    "    evaluation = evaluate_model(cnn, criterion, train_data, test_data, batch_size)\n",
    "    cnn.train()\n",
    "    print(\n",
    "        f'[{epoch + 1:3}] ' \n",
    "        f'train loss: {evaluation[0][0]:.4f}, train accuracy: {100 * evaluation[0][1]:.3f}%, '\n",
    "        f'test loss: {evaluation[1][0]:.4f}, test accuracy: {100 * evaluation[1][1]:.3f}%'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, input_len, embedding_dim):\n",
    "        super(CNN, self).__init__()\n",
    "        self.input_len     = input_len\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embedding     = nn.Embedding(vocab_size + 1, embedding_dim)\n",
    "        self.conv1         = nn.Conv1d(embedding_dim, 64, 3, padding = 1)\n",
    "        self.bn1           = nn.BatchNorm1d(64)\n",
    "        self.dropout1      = nn.Dropout(p = 0.8)\n",
    "        self.conv2         = nn.Conv1d(64 , 64 , 3, padding = 1)\n",
    "        self.bn2           = nn.BatchNorm1d(64)\n",
    "        self.dropout2      = nn.Dropout(p = 0.8)\n",
    "        self.conv3         = nn.Conv1d(64 , 128, 3, padding = 1)\n",
    "        self.bn3           = nn.BatchNorm1d(128)\n",
    "        self.dropout3      = nn.Dropout(p = 0.8)\n",
    "        self.conv4         = nn.Conv1d(128, 128, 3, padding = 1)\n",
    "        self.bn4           = nn.BatchNorm1d(128)\n",
    "        self.dropout4      = nn.Dropout(p = 0.8)\n",
    "        self.linear1       = nn.Linear(128 * 9, 256)\n",
    "        self.bn5           = nn.BatchNorm1d(256)\n",
    "        self.dropout5      = nn.Dropout(p = 0.8)\n",
    "        self.linear2       = nn.Linear(256, 256)\n",
    "        self.bn6           = nn.BatchNorm1d(256)\n",
    "        self.dropout6      = nn.Dropout(p = 0.8)\n",
    "        self.linear3       = nn.Linear(256, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.transpose(1, 2).contiguous()\n",
    "        x = self.dropout1(self.bn1(F.relu(self.conv1(x))))\n",
    "        x = self.dropout2(self.bn2(F.relu(self.conv2(x))))\n",
    "        x = F.avg_pool1d(x, 2)\n",
    "        x = self.dropout3(self.bn3(F.relu(self.conv3(x))))\n",
    "        x = self.dropout4(self.bn4(F.relu(self.conv4(x))))\n",
    "        x = F.avg_pool1d(x, 2)\n",
    "        x = x.view(-1, 9 * 128)\n",
    "        x = self.dropout5(self.bn5(F.relu(self.linear1(x))))\n",
    "        x = self.dropout6(self.bn6(F.relu(self.linear2(x))))\n",
    "        x = F.log_softmax(self.linear3(x), dim = 1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = load_data(vocab_size, input_len)\n",
    "cnn                   = CNN(vocab_size, input_len, embedding_dim)\n",
    "cnn                   = cnn.cuda() if cuda else cnn\n",
    "criterion             = nn.NLLLoss()\n",
    "optimizer             = optim.Adam(cnn.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1] train loss: 0.6934, train accuracy: 49.955%, test loss: 0.6934, test accuracy: 49.903%\n",
      "\t[  1000] running_loss: 0.6997\n",
      "\t[  2000] running_loss: 0.6765\n",
      "\t[  3000] running_loss: 0.6022\n",
      "\t[  4000] running_loss: 0.5419\n",
      "[  2] train loss: 0.4797, train accuracy: 77.822%, test loss: 0.4906, test accuracy: 77.182%\n",
      "\t[  1000] running_loss: 0.4908\n",
      "\t[  2000] running_loss: 0.4807\n",
      "\t[  3000] running_loss: 0.4730\n",
      "\t[  4000] running_loss: 0.4672\n",
      "[  3] train loss: 0.4215, train accuracy: 81.196%, test loss: 0.4469, test accuracy: 79.565%\n",
      "\t[  1000] running_loss: 0.4445\n",
      "\t[  2000] running_loss: 0.4416\n",
      "\t[  3000] running_loss: 0.4415\n",
      "\t[  4000] running_loss: 0.4395\n",
      "[  4] train loss: 0.3977, train accuracy: 82.445%, test loss: 0.4403, test accuracy: 80.029%\n",
      "\t[  1000] running_loss: 0.4180\n",
      "\t[  2000] running_loss: 0.4224\n",
      "\t[  3000] running_loss: 0.4227\n",
      "\t[  4000] running_loss: 0.4212\n",
      "[  5] train loss: 0.3791, train accuracy: 83.438%, test loss: 0.4352, test accuracy: 80.330%\n",
      "\t[  1000] running_loss: 0.4050\n",
      "\t[  2000] running_loss: 0.4048\n",
      "\t[  3000] running_loss: 0.4085\n",
      "\t[  4000] running_loss: 0.4086\n",
      "[  6] train loss: 0.3636, train accuracy: 84.246%, test loss: 0.4333, test accuracy: 80.371%\n",
      "\t[  1000] running_loss: 0.3937\n",
      "\t[  2000] running_loss: 0.3952\n",
      "\t[  3000] running_loss: 0.3983\n",
      "\t[  4000] running_loss: 0.3982\n",
      "[  7] train loss: 0.3572, train accuracy: 84.628%, test loss: 0.4365, test accuracy: 80.268%\n",
      "\t[  1000] running_loss: 0.3833\n",
      "\t[  2000] running_loss: 0.3853\n",
      "\t[  3000] running_loss: 0.3886\n",
      "\t[  4000] running_loss: 0.3912\n",
      "[  8] train loss: 0.3475, train accuracy: 85.122%, test loss: 0.4394, test accuracy: 80.272%\n",
      "\t[  1000] running_loss: 0.3746\n",
      "\t[  2000] running_loss: 0.3794\n",
      "\t[  3000] running_loss: 0.3801\n",
      "\t[  4000] running_loss: 0.3855\n",
      "[  9] train loss: 0.3401, train accuracy: 85.488%, test loss: 0.4386, test accuracy: 80.237%\n",
      "\t[  1000] running_loss: 0.3693\n",
      "\t[  2000] running_loss: 0.3712\n",
      "\t[  3000] running_loss: 0.3744\n",
      "\t[  4000] running_loss: 0.3784\n",
      "[ 10] train loss: 0.3377, train accuracy: 85.684%, test loss: 0.4398, test accuracy: 80.147%\n",
      "\t[  1000] running_loss: 0.3609\n",
      "\t[  2000] running_loss: 0.3673\n",
      "\t[  3000] running_loss: 0.3676\n",
      "\t[  4000] running_loss: 0.3739\n",
      "[ 11] train loss: 0.3311, train accuracy: 85.965%, test loss: 0.4463, test accuracy: 80.047%\n"
     ]
    }
   ],
   "source": [
    "print_model_evaluation(cnn, 0, criterion, train_data, test_data, batch_size)\n",
    "for epoch in range(epochs):\n",
    "    total_loss   = 0\n",
    "    running_loss = 0\n",
    "    for i, (_, inputs, targets) in enumerate(batch_generator(train_data, batch_size)):\n",
    "        optimizer.zero_grad()\n",
    "        inputs        = Variable(inputs)\n",
    "        targets       = Variable(targets)\n",
    "        inputs        = inputs.cuda() if cuda else inputs\n",
    "        targets       = targets.cuda() if cuda else targets\n",
    "        predictions   = cnn(inputs)\n",
    "        loss          = criterion(predictions, targets)\n",
    "        loss_value    = loss.cpu().data[0]\n",
    "        running_loss += loss_value\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % print_every == print_every - 1:\n",
    "            print(f'\\t[{i + 1:6}] running_loss: {running_loss / print_every:.4f}')\n",
    "            running_loss = 0\n",
    "\n",
    "    print_model_evaluation(cnn, epoch + 1, criterion, train_data, test_data, batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
